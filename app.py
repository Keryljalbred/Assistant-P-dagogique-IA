import streamlit as st
from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM
from langchain.chains import RetrievalQA
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.llms import HuggingFacePipeline
from deep_translator import GoogleTranslator
import sqlite3
from datetime import datetime
import re
import os

# ============================
# Initialisation base SQLite
# ============================
def init_db():
    conn = sqlite3.connect("history.db")
    cursor = conn.cursor()
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS history (
            timestamp TEXT,
            question TEXT,
            answer TEXT,
            summary TEXT,
            topic TEXT
        )
    ''')
    conn.commit()
    conn.close()

# Enregistrement dans l'historique

def log_interaction(question, answer, summary, topic):
    conn = sqlite3.connect("history.db")
    cursor = conn.cursor()
    cursor.execute("INSERT INTO history VALUES (?, ?, ?, ?, ?)",
                   (datetime.now().isoformat(), question, answer, summary, topic))
    conn.commit()
    conn.close()

# === D√©tection langue + traduction
def detect_and_translate(text, target_lang="en"):
    keywords_fr = ["qu'est-ce", "d√©finis", "c'est quoi", "en quoi", "d√©finition", "algorithme", "r√©seau", "python", "fonction", "logique"]
    if any(k in text.lower() for k in keywords_fr):
        return GoogleTranslator(source='auto', target=target_lang).translate(text), 'fr'
    else:
        return text, 'en'
    
# Retraduction de la r√©ponse en fran√ßais si n√©cessaire
def retranslate_if_needed(answer, original_lang):
    if original_lang == 'fr':
        return GoogleTranslator(source='en', target='fr').translate(answer)
    return answer

def detect_topic(question):
    topics = {
        "programmation": ["python", "code", "boucle", "fonction"],
        "maths": ["alg√®bre", "matrice", "nombre", "logique"],
        "algorithmique": ["algorithme", "complexit√©", "tri", "pseudo-code"],
        "Reseau": ["reseau","bit"]
    }
    for topic, keywords in topics.items():
        if any(k in question.lower() for k in keywords):
            return topic
    return "autre"

# Nettoyage basique du texte g√©n√©r√©

def clean_output(text):
    text = re.sub(r"[^a-zA-Z0-9 √†-√º,.!?()\-]", '', text)
    return text.strip().capitalize()

# R√©sum√© automatique d‚Äôun texte via Flan-T5

def generate_summary(text, pipe):
    prompt = f"R√©sume ce texte en fran√ßais : {text}"
    return pipe(prompt, max_length=100, do_sample=False)[0]['generated_text']

# === Nettoyage simple
def clean_output(text):
    text = re.sub(r"[^a-zA-Z√Ä-√ø0-9 ,.'\"()!?-]", '', text)
    return text.strip().capitalize()

# === Chargement du mod√®le local
with st.spinner("Chargement du mod√®le FLAN-T5..."):
    tokenizer = AutoTokenizer.from_pretrained("google/flan-t5-base")
    model = AutoModelForSeq2SeqLM.from_pretrained("google/flan-t5-base")
    pipe = pipeline("text2text-generation", model=model, tokenizer=tokenizer, max_length=256)
    local_llm = HuggingFacePipeline(pipeline=pipe)

# === Chargement FAISS & embeddings
index_path = "index/faiss_index"
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vectordb = FAISS.load_local(index_path, embeddings=embedding_model, allow_dangerous_deserialization=True)
retriever = vectordb.as_retriever()

# === Interface utilisateur Streamlit
st.set_page_config(page_title="Assistant P√©dagogique IA", layout="wide")
st.title("üéì Assistant p√©dagogique IA")
st.write("Pose ta question sur un cours üìö")
init_db()

# Champ de saisie utilisateur

question = st.text_input("üí¨ Ta question ici :")

if question:
    with st.spinner("Recherche et g√©n√©ration..."):
        question_en, lang = detect_and_translate(question, target_lang="en")
        topic = detect_topic(question)


        qa_chain_local = RetrievalQA.from_chain_type(llm=local_llm, retriever=retriever, return_source_documents=True)
        result = qa_chain_local({"query": question_en})
        answer = clean_output(result["result"])
        
        # Fallback si r√©ponse trop courte ou insatisfaisante
        if len(answer.split()) < 5 or answer.lower() in ["internet", "python", "bit"]:
            st.warning("R√©ponse locale trop courte. Appel √† OpenAI GPT-3.5 en secours...")
            openai_key = os.getenv("OPENAI_API_KEY")
            if openai_key:
                try:
                    openai_llm = OpenAI(temperature=0.3, max_tokens=300, openai_api_key=openai_key, model_name="gpt-3.5-turbo")
                    qa_chain_gpt = RetrievalQA.from_chain_type(llm=openai_llm, retriever=retriever, return_source_documents=True)
                    result = qa_chain_gpt({"query": question})
                    answer = result["result"]
                except Exception as e:
                    st.error(f"Erreur OpenAI : {e}")
            else:
                st.error("‚ùå Cl√© OpenAI manquante. Ajoutez-la √† `.env` ou aux variables d‚Äôenvironnement.")

#  Retraduction si n√©cessaire
        answer = retranslate_if_needed(answer, lang)
        summary = generate_summary(answer, pipe)

# Affichage des r√©sultats
        st.markdown(f"\nüí° **R√©ponse :** {answer}")
        st.markdown(f"üìù **R√©sum√© :** {summary}")
        st.markdown(f"üìö **Th√®me d√©tect√© :** {topic}")

# Enregistrement dans l‚Äôhistorique
        log_interaction(question, answer, summary, topic)

# Affichage des sources utilis√©es
        with st.expander("üìÑ Sources utilis√©es :"):
            for doc in result["source_documents"]:
                st.write("‚Ä¢", doc.metadata.get("source", "inconnu"))

# ============================
# Affichage de l‚Äôhistorique
# ============================
st.markdown("---")
st.subheader("üìú Historique des interactions")

if st.button("Voir l'historique"):
    conn = sqlite3.connect("history.db")
    cursor = conn.cursor()
    cursor.execute("SELECT timestamp, question, answer, summary, topic FROM history ORDER BY timestamp DESC")
    rows = cursor.fetchall()
    conn.close()

    if rows:
        for row in rows:
            st.markdown(f"""
            üïí **Date** : {row[0]}  
            üí¨ **Question** : {row[1]}  
            üí° **R√©ponse** : {row[2]}  
            üìù **R√©sum√©** : {row[3]}  
            üìö **Th√®me** : {row[4]}  
            ---
            """)
    else:
        st.info("Aucune interaction enregistr√©e pour le moment.")